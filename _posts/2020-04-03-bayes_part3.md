---
keywords: fastai
description: 1次元ガウス分布のベイズ推論を実践する
title: ベイズ勉強会 Part 3 1次元ガウス分布のベイズ推論
toc: true 
badges: false
comments: true
categories: [bayes]
image: images/dag1.png
nb_path: _notebooks/2020-04-03-bayes_part3.ipynb
layout: notebook
---

<!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-04-03-bayes_part3.ipynb
-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>参考図書 {% fn 1 %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="1&#27425;&#20803;&#12460;&#12454;&#12473;&#20998;&#24067;">1&#27425;&#20803;&#12460;&#12454;&#12473;&#20998;&#24067;<a class="anchor-link" href="#1&#27425;&#20803;&#12460;&#12454;&#12473;&#20998;&#24067;"> </a></h1><p>1次元のガウス分布(以下本ページでは単に「ガウス分布」と呼ぶ)は、次の確率密度関数で表される$x \in \mathbb{R}$を生成する確率分布である。</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include important.html content='1次元ガウス分布の確率密度関数$$\mathcal{N}(x|\mu,\sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}} exp\{-\frac{(x-\mu)^2}{2\sigma^2}\}$$' %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>パラメータは$\mu \in \mathbb{R}, \sigma^2 \in \mathbb{R^+}$の2つで、それぞれ平均と分散である。精度パラメータ$\lambda = \sigma^{-2}$を用いて書くこともある。精度で書くと以下のようになる。</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include important.html content='精度で表した1次元ガウス分布の確率密度関数$$\mathcal{N}(x|\mu,\lambda^{-1}) = \frac{1}{\sqrt{2 \pi}} \lambda^{\frac{1}{2}}exp\{-\frac{1}{2}(x-\mu)^2 \lambda\}$$' %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>以下、観測されたデータ$\mathcal{D}=\{x_1,\dots,x_N\}$の各点$x_n$と未知の観測$x_*$は同じ1次元ガウス分布から独立に生成されたと仮定してベイズ推論を行う。</p>
<h1 id="&#24179;&#22343;&#12364;&#26410;&#30693;&#12289;&#31934;&#24230;&#12364;&#26082;&#30693;&#12398;&#22580;&#21512;">&#24179;&#22343;&#12364;&#26410;&#30693;&#12289;&#31934;&#24230;&#12364;&#26082;&#30693;&#12398;&#22580;&#21512;<a class="anchor-link" href="#&#24179;&#22343;&#12364;&#26410;&#30693;&#12289;&#31934;&#24230;&#12364;&#26082;&#30693;&#12398;&#22580;&#21512;"> </a></h1><p>ガウス分布の2つのパラメータのうち、精度パラメータ$\lambda$が既知である状況でベイズ推論を行う。</p>
<h2 id="&#12514;&#12487;&#12523;&#12398;&#27083;&#31689;">&#12514;&#12487;&#12523;&#12398;&#27083;&#31689;<a class="anchor-link" href="#&#12514;&#12487;&#12523;&#12398;&#27083;&#31689;"> </a></h2><p>平均$\mu$だけが未知という条件で、尤度関数をガウス分布にした場合、そのパラメータ$\mu$の事前分布はどうすればよいだろうか。$\mu$の条件は$\mu \in \mathbb{R}$であることのみであり、実数を1つ出力する分布を事前分布とすればベイズ推論ができそうだ。このような分布は様々だが、1次元ガウス分布を用いることで事後分布も1次元ガウス分布となることが知られている。つまり尤度関数が1次元ガウス分布の場合の共役事前分布は1次元ガウス分布である。これを用いて同時分布を構築すると以下のようになる。</p>
$$
\begin{eqnarray}
    p(\mathcal{D},x_*,\mu) &amp;=&amp; p(\mathcal{D}|\mu)p(x_*|\mu)p(\mu) \\
    p(\mathcal{D}|\mu) &amp;=&amp; \Pi_{n=1}^{N} \mathcal{N}(x_n|\mu, \lambda^{-1}) \\
    p(x_*|\mu) &amp;=&amp; \mathcal{N}(x_*|\mu, \lambda^{-1}) \\
    p(\mu) &amp;=&amp; \mathcal{N}(\mu|m, \lambda_{\mu}^{-1})
\end{eqnarray}
$$<p>$p(\mu)$の$m \in \mathbb{R}, \lambda_{\mu} \in \mathbb{R^+}$は固定されたハイパーパラメータである。</p>
<h2 id="&#20107;&#24460;&#20998;&#24067;&#12398;&#25512;&#35542;">&#20107;&#24460;&#20998;&#24067;&#12398;&#25512;&#35542;<a class="anchor-link" href="#&#20107;&#24460;&#20998;&#24067;&#12398;&#25512;&#35542;"> </a></h2><p>事後分布$p(\mu|\mathcal{D})$を求める。ベイズの定理から次のように書ける。分母は形状には関わらないので省く。</p>
$$
\begin{eqnarray}
    p(\mu|\mathcal{D}) &amp;\propto&amp; p(\mathcal{D}|\mu) p(\mu) \\
    &amp;=&amp; \{\Pi_{n=1}^{N} p(x_n|\mu)\}p(\mu) \\
    &amp;=&amp; \{\Pi_{n=1}^{N} \mathcal{N}(x_n|\mu, \lambda^{-1})\} \mathcal{N}(\mu|m,\lambda_{\mu}^{-1})
\end{eqnarray}
$$<p>対数をとると、</p>
$$
\begin{eqnarray}
    p(\mu|\mathcal{D}) &amp;=&amp; \Sigma_{n=1}^{N} \ln \mathcal{N}(x_n|\mu,\lambda^{-1}) + \ln \mathcal{N}(\mu|m,\lambda_{\mu}^{-1}) + const. \\
    &amp;=&amp; \Sigma_{n=1}^{N} \{-\frac{1}{2}(x_n-\mu)^2 \lambda\} + \{-\frac{1}{2}(\mu-m)^2 \lambda_{\mu}\} + const. \\
    &amp;=&amp; -\frac{1}{2}\left[\Sigma_{n=1}^{N}\{(x_n^2 -2x_n\mu + \mu^2)\lambda\} + (\mu^2 - 2\mu m + m^2)\lambda_{\mu}\right] + const. \\
    &amp;=&amp; -\frac{1}{2}\{(N\lambda + \lambda_{\mu})\mu^2 - 2(\Sigma_{n=1}^{N} x_n \lambda + m \lambda_{\mu})\mu\} + const.
\end{eqnarray}
$$<p>最後の変形では$\mu$に関わらない値は$const.$に吸収させている。これを平方完成するとガウス分布の形になっていることがわかるがここでは結果から逆算的に求める。事後分布が次のようなガウス分布で書けるとする。</p>
$$
p(\mu|\mathcal{D}) = \mathcal{N}(\mu|\hat{m},\hat{\lambda_{\mu}}^{-1})
$$<p>対数をとると</p>
$$
\begin{eqnarray}
    \ln p(\mu|\mathcal{D}) &amp;=&amp; -\frac{1}{2}(\mu-\hat{m})^2 \hat{\lambda_{\mu}} \\
    &amp;=&amp; -\frac{1}{2}\{\hat{\lambda_{\mu}} \mu^2 - 2\hat{m} \hat{\lambda_{\mu}} \mu\} + const.
\end{eqnarray}
$$<p>となるので、事後分布のパラメータ$\hat{m},\hat{\lambda_{\mu}}$は次のように求まる。</p>
$$
\begin{eqnarray}
    \hat{\lambda_{\mu}} &amp;=&amp; N \lambda + \lambda_{\mu} \\
    \hat{m} &amp;=&amp; \frac{\lambda \Sigma_{n=1}^{N} x_n + \lambda_{\mu}m}{\hat{\lambda_{\mu}}}
\end{eqnarray}
$$<p>以上で事後分布の推論が完了した。</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include note.html content='更新された精度パラメータは、ハイパーパラメータに$N\lambda$だけ加えたものでありデータ数$N$が大きくなる程精度が上がる、すなわち事後分布のばらつきが小さくなりかつハイパーパラメータの影響が小さくなることを示している。平均パラメータはハイパーパラメータ$m$と$\Sigma{n=1}^{N}x_n$の重み付き和の形になっておりこれもデータ数が増えるほどハイパーパラメータの影響を受けにくくなることがわかる。' %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="&#20104;&#28204;&#20998;&#24067;&#12398;&#31639;&#20986;">&#20104;&#28204;&#20998;&#24067;&#12398;&#31639;&#20986;<a class="anchor-link" href="#&#20104;&#28204;&#20998;&#24067;&#12398;&#31639;&#20986;"> </a></h2><p>モデル全体の事後分布をパラメータ$\mu$で周辺化することで未知の観測$x_*$に対する予測分布が得られる。ハット記号を付けるのが面倒なので、学習していない事前分布から予測分布を算出し、後で更新されたパラメータを代入してみることにする。</p>
$$
\begin{eqnarray}
    p(x_*) &amp;=&amp; \int p(x_*|\mu) p(\mu) d\mu \\
    &amp;=&amp; \int \mathcal{N}(x_*|\mu,\lambda^{-1})\mathcal{N}(\mu|m,\lambda_{\mu}^{-1}) d\mu
\end{eqnarray}
$$<p>これを直接計算するのは大変なので、ベイズの定理と対数をうまく使ってみる。ベイズの定理から、</p>
$$
p(\mu|x_*) = \frac{p(x_*|\mu)p(\mu)}{p(x_*)}
$$<p>$p(x_*)$を左辺に置いて対数をとると、</p>
$$
\ln p(x_*) = \ln p(x_*|\mu) - \ln p(\mu|x_*) + const.
$$<p>$\ln p(\mu)$は$x_*$には関係ないので$const.$とした。$p(\mu|x_*)$は$p(\mu|\mathcal{D})$に$\mathcal{D}=x_*$とすれば求まる。</p>
$$
\begin{eqnarray}
    p(\mu|x_*) &amp;=&amp; \mathcal{N}(\mu|m(x_*), (\lambda + \lambda_{\mu})^{-1}) \\
    ただし　m(x_*) &amp;=&amp; \frac{\lambda x_* + \lambda_{\mu}m}{\lambda + \lambda_{\mu}}
\end{eqnarray}
$$<p>これと$p(x_*|\mu) = \mathcal{N}(\mu, \lambda^{-1})$を代入すると</p>
$$
\begin{eqnarray}
    \ln p(x_*) &amp;=&amp; \ln p(x_*|\mu) - \ln p(\mu|x_*) + const. \\
    &amp;=&amp; -\frac{1}{2} \{ \lambda (x_* - \mu)^2 - (\lambda + \lambda_{\mu})(\mu - m(x_*))^2 \} + const. \\
    &amp;=&amp; -\frac{1}{2} \{ \lambda x_*^2 - 2 \lambda \mu x_* - \frac{\lambda^2 x_*^2 + 2 \lambda \lambda_{\mu} m x_*}{\lambda + \lambda_{\mu}} + 2 \lambda \mu x_* \} + const. \\
    &amp;=&amp; -\frac{1}{2} \{ \frac{\lambda^2 x_*^2 + \lambda \lambda_{\mu} x_*^2 - \lambda^2 x_*^2 - 2 \lambda \lambda_{\mu} m x_*}{\lambda + \lambda_{\mu}}\} + const. \\
    &amp;=&amp; -\frac{1}{2} \{ \frac{\lambda \lambda_{\mu}}{\lambda + \lambda_{\mu}} x_*^2 - \frac{2m\lambda\lambda_{\mu}}{\lambda + \lambda_{\mu}} x_* \} + const.
\end{eqnarray}
$$<p>$x_*$の2次関数の形にできたので事後分布と同じく逆算的に計算すると、予測分布$p(x_*)$は</p>
$$
\begin{eqnarray}
    p(x_*) &amp;=&amp; \mathcal{N} (x_* | \mu_*, \lambda_{*}^{-1}) \\
    ただし　\lambda_* &amp;=&amp; \frac{\lambda \lambda_{\mu}}{\lambda + \lambda_{\mu}} \\
    \mu_* &amp;=&amp; m
\end{eqnarray}
$$<p>となる。これに$\hat{m}, \hat{\lambda_{\mu}}$を代入することで学習した後の予測分布$p(x_*|\mathcal{D})$は</p>
$$
\begin{eqnarray}
    p(x_*) &amp;=&amp; \mathcal{N} (x_* | \mu_*, \lambda_{*}^{-1}) \\
    ただし　\lambda_* &amp;=&amp; \frac{\lambda (N\lambda + \lambda_{\mu})}{\lambda + (N\lambda + \lambda_{\mu})} \\
    \mu_* &amp;=&amp; \frac{\lambda \Sigma_{n=1}^{N} x_n + \lambda_{\mu}m}{N\lambda + \lambda_{\mu}}
\end{eqnarray}
$$<p>と求まる。</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include note.html content='精度から見た結果の意味' %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote><blockquote><p>精度について逆数をとると意味がわかりやすい。</p>
$$\lambda_*^{-1} = \lambda^{-1} + \hat{\lambda_{\mu}}^{-1}$$<p>精度の逆数は分散なので、この式は「予測分布の分散は観測分布の分散と事後分布の分散の和である」という意味になる。</p>
<p>今回は観測分布の分散が実際の分散に等しいことを仮定しているので、データ数が増え事後分布の分散が小さくなれば予測分布は実際の分布の分散とほぼ一致する。</p>
</blockquote>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="&#24179;&#22343;&#12364;&#26082;&#30693;&#12289;&#31934;&#24230;&#12364;&#26410;&#30693;&#12398;&#22580;&#21512;">&#24179;&#22343;&#12364;&#26082;&#30693;&#12289;&#31934;&#24230;&#12364;&#26410;&#30693;&#12398;&#22580;&#21512;<a class="anchor-link" href="#&#24179;&#22343;&#12364;&#26082;&#30693;&#12289;&#31934;&#24230;&#12364;&#26410;&#30693;&#12398;&#22580;&#21512;"> </a></h1><p>今度は、平均パラメータ$\mu$が既知で、精度パラメータ$\lambda$が未知の場合でベイズ推論を行う。</p>
<h2 id="&#12514;&#12487;&#12523;&#12398;&#27083;&#31689;">&#12514;&#12487;&#12523;&#12398;&#27083;&#31689;<a class="anchor-link" href="#&#12514;&#12487;&#12523;&#12398;&#27083;&#31689;"> </a></h2><p>精度パラメータ$\lambda$は正の実数である必要がある。正の実数を出力する確率分布にはガンマ分布があり、平均既知精度未知の場合の1次元ガウス分布の共役事前分布として知られている。事前分布にガンマ分布を採用すると次のようにモデル構築することになる。</p>
$$
\begin{eqnarray}
    p(\mathcal{D},x_*,\lambda) &amp;=&amp; p(\mathcal{D}|\lambda)p(x_*|\lambda)p(\lambda) \\
    p(\mathcal{D}|\lambda) &amp;=&amp; \Pi_{n=1}^{N} \mathcal{N}(x_n|\mu, \lambda^{-1}) \\
    p(x_*|\lambda) &amp;=&amp; \mathcal{N}(x_*|\mu, \lambda^{-1}) \\
    p(\lambda) &amp;=&amp; Gam(\lambda|a, b)
\end{eqnarray}
$$<p>このモデルのハイパーパラメータは$a,b$である。$\mu$は既知の値という設定だが、$\mu$をハイパーパラメータとして推論していると考えることもできる。</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include important.html content='ガンマ分布' %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote><blockquote><p>ガンマ分布は$a, b \in \mathbb{R^+}$をパラメータに持ち、正の実数を生成する確率分布である。ガンマ分布の確率密度関数は次の通り。</p>
$$Gam(\lambda|a,b) = C_G(a,b)\lambda^{a-1}e^{-b\lambda}$$<p>$C_G(a,b)$は正規化係数であり、</p>
$$C_G(a,b) = \frac{b^a}{\Gamma(a)}$$
</blockquote>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="&#20107;&#24460;&#20998;&#24067;&#12398;&#25512;&#35542;">&#20107;&#24460;&#20998;&#24067;&#12398;&#25512;&#35542;<a class="anchor-link" href="#&#20107;&#24460;&#20998;&#24067;&#12398;&#25512;&#35542;"> </a></h2><p>事後分布$p(\lambda|\mathcal{D})$を推論する。ベイズの定理から</p>
$$
\begin{eqnarray}
    p(\lambda|\mathcal{D}) &amp;\propto&amp; p(\mathcal{D}|\lambda)p(\lambda) \\
    &amp;=&amp; \{ \Pi_{n=1}^{N} p(x_n|\lambda) \} p(\lambda) \\
    &amp;=&amp; \{ \Pi_{n=1}^{N} \mathcal{N}(x_n|\mu, \lambda^{-1}) \} Gam(\lambda|a,b)
\end{eqnarray}
$$<p>対数をとる。</p>
$$
\begin{eqnarray}
    \ln p(\lambda|\mathcal{D}) &amp;=&amp; \Sigma_{n=1}^{N} \ln \mathcal{N}(x_n|\mu,\lambda^{-1}) + \ln Gam(\lambda|a,b) + const. \\
    &amp;=&amp; \Sigma_{n=1}^{N}\{\frac{1}{2} \ln \lambda - \frac{(x_n-\mu)^2 \lambda}{2}\} + (a-1)\ln \lambda -b\lambda + const. \\
    &amp;=&amp; (\frac{N}{2}+a-1)\ln \lambda - \{\frac{1}{2}\Sigma_{n=1}^{N}(x_n-\mu)^2 + b\}\lambda + const.
\end{eqnarray}
$$<p>対数を戻せばこれはガンマ分布となることがわかる。</p>
$$
\begin{eqnarray}
    p(\lambda|\mathcal{D}) &amp;=&amp; Gam(\lambda|\hat{a},\hat{b}) \\
    ただし　\hat{a} &amp;=&amp; \frac{N}{2} + a \\
    \hat{b} &amp;=&amp; \frac{1}{2} \Sigma_{n=1}^{N} (x_n-\mu)^2 + b
\end{eqnarray}
$$<h2 id="&#20104;&#28204;&#20998;&#24067;&#12398;&#31639;&#20986;">&#20104;&#28204;&#20998;&#24067;&#12398;&#31639;&#20986;<a class="anchor-link" href="#&#20104;&#28204;&#20998;&#24067;&#12398;&#31639;&#20986;"> </a></h2><p>事後分布の形状が事前分布と同じなので、学習前の予測分布$p(x_*)$を計算すれば、学習後の$\hat{a},\hat{b}$を代入するだけで$p(x_*|\mathcal{D})$がわかる。</p>
$$
p(x_*) = \int p(x_*|\lambda)p(\lambda) d\lambda
$$<p>を直接計算せずにベイズの定理と対数計算で簡単に計算してみる。</p>
$$
p(\lambda|x_*) = \frac{p(x_*|\lambda)p(\lambda)}{p(x_*)}
$$<p>対数をとり、$p(\lambda)$を定数にまとめれば</p>
$$
\ln p(x_*) = \ln p(x_*|\lambda) - \ln p(\lambda|x_*) + const.
$$<p>$\ln p(\lambda|x_*)$は事後分布の形に合わせれば</p>
$$
\begin{eqnarray}
    p(\lambda|x_*) &amp;=&amp; Gam(\lambda|\frac{1}{2}+a,b(x_*)) \\
    ただし　b(x_*) &amp;=&amp; \frac{1}{2}(x_*-\mu)^2 + b
\end{eqnarray}
$$<p>と書ける。これを代入して</p>
$$
\begin{eqnarray}
    \ln p(x_*) &amp;=&amp; \ln \mathcal{N}(x_*|\mu,\lambda^{-1}) - \ln Gam(\lambda|\frac{1}{2}+a,b(x_*)) + cosnt. \\
    &amp;=&amp; \frac{1}{2} \ln \lambda - \frac{(x_*-\mu)^2 \lambda}{2} - (a-\frac{1}{2})\ln \lambda + \{\frac{1}{2}(x_*-\mu)^2 + b \} \lambda - \ln C_G(a+\frac{1}{2}, \frac{1}{2}(x_*-\mu)^2 + b) + const. \\
    &amp;=&amp; - (a+\frac{1}{2})\ln \{\frac{1}{2}(x_*-\mu)^2 + b\} + \Gamma(a+\frac{1}{2}) + const. \\
    &amp;=&amp; - \frac{2a+1}{2} \ln \{ 1 + \frac{1}{2b}(x_*-\mu)^2 \} + const.
\end{eqnarray}
$$<p>となる。途中、$x_*$を含まない項は$const.$に吸収させている。また、$\lambda$に関わる項は消えている。この結果は1次元のStudentのt分布に対数をとったものと同じ形になっている。</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include important.html content='1次元のStudentのt分布' %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote><blockquote><p>1次元のStudentのt分布は次の確率密度関数で表される。</p>
$$St(x|\mu_s,\lambda_s,\nu_s) = \frac{\Gamma(\frac{\nu_s + 1}{2})}{\Gamma(\frac{\nu_s}{2})}(\frac{\lambda_s}{\pi \nu_s})^{\frac{1}{2}}\{ 1 + \frac{\lambda_s}{\nu_s} (x-\mu_s)^2 \}^{-\frac{\nu_s+1}{2}}$$<p>対数をとり$x$に関わらない部分をconst.にまとめると</p>
$$\ln St(x|\mu_s,\lambda_s,\nu_s) = -\frac{\nu_s+1}{2} \ln \{ 1 + \frac{\lambda_s}{\nu_s} (x-\mu_s)^2 \} + const.$$
</blockquote>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>対数t分布との対応を見れば、予測分布は次のように書けることがわかる。</p>
$$
\begin{eqnarray}
    p(x_*) &amp;=&amp; St(x_*|\mu_s,\lambda_s,\nu_s) \\
    ただし　\mu_s &amp;=&amp; \mu \\
    \lambda_s &amp;=&amp; \frac{a}{b} \\
    \nu_s &amp;=&amp; 2a
\end{eqnarray}
$$<p>学習により更新された$\hat{a},\hat{b}$を代入すると次のようになる。</p>
$$
\begin{eqnarray}
    p(x_*|\mathcal{D}) &amp;=&amp; St(x_*|\mu_s,\lambda_s,\nu_s) \\
    ただし　\mu_s &amp;=&amp; \mu \\
    \lambda_s &amp;=&amp; \frac{N+2a}{\Sigma_{n=1}^{N} (x_n-\mu)^2 + 2b} \\
    \nu_s &amp;=&amp; N + 2a
\end{eqnarray}
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="&#24179;&#22343;&#12392;&#31934;&#24230;&#12364;&#12392;&#12418;&#12395;&#26410;&#30693;&#12398;&#22580;&#21512;">&#24179;&#22343;&#12392;&#31934;&#24230;&#12364;&#12392;&#12418;&#12395;&#26410;&#30693;&#12398;&#22580;&#21512;<a class="anchor-link" href="#&#24179;&#22343;&#12392;&#31934;&#24230;&#12364;&#12392;&#12418;&#12395;&#26410;&#30693;&#12398;&#22580;&#21512;"> </a></h1><p>執筆中</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{{ '<a href="https://www.kspub.co.jp/book/detail/1538320.html">『ベイズ推論による機械学習入門』(須山敦志、講談社)</a>' | fndetail: 1 }}</p>

</div>
</div>
</div>
</div>
 

